{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaebe265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0db4b54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.73s)\n",
      "creating index...\n",
      "index created!\n",
      "Annotation for ID 874893 : [{'image_id': 262136, 'id': 874893, 'caption': 'a photo of a person'}]\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Initialize COCO API for caption annotations\n",
    "annFile = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/ProbVLM/Datasets/coco/captions_train2014_extra.json'  # Path to COCO annotations file\n",
    "coco = COCO(annFile)\n",
    "\n",
    "# Get all available annotation IDs\n",
    "all_annotation_ids = set(coco.getAnnIds())\n",
    "sorted_ann_id = sorted(all_annotation_ids)\n",
    "\n",
    "# Select a valid annotation ID (first one as an example)\n",
    "valid_annotation_id = sorted_ann_id[-1]  # This should be a valid ID\n",
    "\n",
    "# Load and print the annotation for the selected ID\n",
    "annotation = coco.loadAnns(valid_annotation_id)\n",
    "print(\"Annotation for ID\", valid_annotation_id, \":\", annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a87bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco2 = COCO('/mimer/NOBACKUP/groups/ulio_inverse/ds-project/ProbVLM/Datasets/coco/captions_val2014_extra.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf104633",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/ProbVLM/Datasets/coco/filtered_ids_train.txt'\n",
    "path2 = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/ProbVLM/Datasets/coco/filtered_ids_val3.txt'\n",
    "path3 = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/ProbVLM/Datasets/coco/filtered_ids_test3.txt'\n",
    "# Step 2: Load IDs from the text file\n",
    "with open(path, 'r') as file:\n",
    "    filtered_cap_ids_train = [int(line.strip()) for line in file]  # Read and convert IDs to integers\n",
    "    \n",
    "with open(path2, 'r') as file:\n",
    "    filtered_cap_ids_val = [int(line.strip()) for line in file]  # Read and convert IDs to integers\n",
    "    \n",
    "with open(path3, 'r') as file:\n",
    "    filtered_cap_ids_test = [int(line.strip()) for line in file]  # Read and convert IDs to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35981b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13135"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filtered_cap_ids_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a439858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_train = coco.loadAnns(filtered_cap_ids_train)\n",
    "img_ids_val = coco2.loadAnns(filtered_cap_ids_val)\n",
    "img_ids_test = coco2.loadAnns(filtered_cap_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7ce0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids_train_1 = set([item['image_id'] for item in img_ids_train])\n",
    "img_ids_val_1 = set([item['image_id'] for item in img_ids_val])\n",
    "img_ids_test_1 = set([item['image_id'] for item in img_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cdd22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/GALS/data/COCO/COCO_gender/biased_split/test.ids.txt'\n",
    "txt_file_path2 = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/GALS/data/COCO/COCO_gender/balanced_split/val_woman.txt'\n",
    "txt_file_path3 = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/GALS/data/COCO/COCO_gender/balanced_split/val_man.txt'\n",
    "# Step 2: Load IDs from the text file\n",
    "with open(txt_file_path, 'r') as file:\n",
    "    text_file_img_ids = {int(line.strip()) for line in file}  # Read and convert IDs to integers\n",
    "    \n",
    "with open(txt_file_path2, 'r') as file:\n",
    "    text_file_img_ids2 = {int(line.strip()) for line in file}  # Read and convert IDs to integers\n",
    "    \n",
    "with open(txt_file_path3, 'r') as file:\n",
    "    text_file_img_ids3 = {int(line.strip()) for line in file}  # Read and convert IDs to integers\n",
    "    \n",
    "combined_img_ids = text_file_img_ids3.union(text_file_img_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c28780b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filtered_img_ids_test))#-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16d067fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_img_ids_test = img_ids_test_1.intersection(text_file_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0033d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_img_ids = img_ids_val_1.intersection(text_file_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b3ca087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_img_ids_train = img_ids_train_1.intersection(text_file_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "386a2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_synonym(caption, synonym_list):\n",
    "    # Convert caption to lowercase\n",
    "    caption = caption.lower()\n",
    "\n",
    "    \n",
    "    # Use regex to match whole words\n",
    "    return any(re.search(r'\\b' + re.escape(word) + r'\\b', caption) for word in synonym_list)\n",
    "\n",
    "woman_word_list_synonyms = ['girl', 'sister', 'mom', 'wife', 'woman',      \\\n",
    "        'bride', 'female', 'lady',  'actress', 'nun', 'girlfriend',        \\\n",
    "        'her', 'she', 'mother', 'daughter', 'businesswoman', 'cowgirl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13c2c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "man_word_list_synonyms = ['boy', 'brother', 'dad', 'husband', 'man',       \\\n",
    "        'groom', 'male','guy', 'dude', 'policeman', 'father',              \\\n",
    "        'son', 'fireman', 'actor','gentleman', 'boyfriend',                \\\n",
    "        'mans', 'his', 'obama', 'businessman', 'he', 'cowboy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ef96969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_woman:  188\n",
      "num_man:  461\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for woman and man synonyms\n",
    "image_results = {}\n",
    "woman_counter = 0\n",
    "man_counter = 0\n",
    "woman_imgs = []\n",
    "man_imgs = []\n",
    "\n",
    "# Iterate through all the image IDs and extract captions\n",
    "for img_id in list(filtered_img_ids_test):  # Removed the [:1] so all images are processed\n",
    "    skip_image = False  # Flag to skip the current image if both types of synonyms are found\n",
    "    \n",
    "    # Get annotation IDs for the current image_id\n",
    "    ann_ids = coco2.getAnnIds(imgIds=img_id)\n",
    "    \n",
    "    # Load the annotations for the current annotation IDs\n",
    "    annotations = coco2.loadAnns(ann_ids)\n",
    "    \n",
    "    woman_found = False\n",
    "    man_found = False\n",
    "    caption = None\n",
    "    \n",
    "    # Iterate through the annotations and extract captions\n",
    "    for annotation in annotations:\n",
    "        caption = annotation['caption']\n",
    "        #print(f\"Image ID {img_id}: {caption}\")  # Print each caption\n",
    "        \n",
    "        # Check if the caption contains any woman synonym\n",
    "        if contains_synonym(caption, woman_word_list_synonyms):\n",
    "            woman_found = True\n",
    "\n",
    "        \n",
    "        # Check if the caption contains any man synonym\n",
    "        if contains_synonym(caption, man_word_list_synonyms):\n",
    "            man_found = True\n",
    "\n",
    "        \n",
    "        # If both woman and man synonyms are found in the same image, skip to next image\n",
    "        if woman_found and man_found:\n",
    "\n",
    "            skip_image = True\n",
    "            break\n",
    "    \n",
    "    if not skip_image:\n",
    "        if woman_found:\n",
    "            woman_counter += 1\n",
    "            woman_imgs.append(img_id)\n",
    "        elif man_found:\n",
    "            man_counter += 1\n",
    "            man_imgs.append(img_id)\n",
    "\n",
    "print('num_woman: ', woman_counter)\n",
    "print('num_man: ', man_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5535b8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[124930,\n",
       " 241668,\n",
       " 28688,\n",
       " 405531,\n",
       " 415770,\n",
       " 114744,\n",
       " 376891,\n",
       " 536653,\n",
       " 84060,\n",
       " 69757,\n",
       " 436350,\n",
       " 98434,\n",
       " 16521,\n",
       " 356505,\n",
       " 522427,\n",
       " 532695,\n",
       " 383211,\n",
       " 424172,\n",
       " 545007,\n",
       " 6393,\n",
       " 354559,\n",
       " 286981,\n",
       " 4359,\n",
       " 211206,\n",
       " 504074,\n",
       " 192782,\n",
       " 489763,\n",
       " 39202,\n",
       " 477483,\n",
       " 168243,\n",
       " 491835,\n",
       " 516416,\n",
       " 479562,\n",
       " 561491,\n",
       " 133470,\n",
       " 29045,\n",
       " 459141,\n",
       " 377239,\n",
       " 192932,\n",
       " 551338,\n",
       " 170436,\n",
       " 61899,\n",
       " 504293,\n",
       " 250344,\n",
       " 20992,\n",
       " 227851,\n",
       " 49688,\n",
       " 127516,\n",
       " 457249,\n",
       " 211491,\n",
       " 105011,\n",
       " 10825,\n",
       " 279149,\n",
       " 469618,\n",
       " 209527,\n",
       " 156282,\n",
       " 266892,\n",
       " 234153,\n",
       " 242365,\n",
       " 164548,\n",
       " 64196,\n",
       " 21198,\n",
       " 563927,\n",
       " 47837,\n",
       " 482021,\n",
       " 199404,\n",
       " 115455,\n",
       " 109316,\n",
       " 488198,\n",
       " 99081,\n",
       " 213773,\n",
       " 152336,\n",
       " 217872,\n",
       " 70426,\n",
       " 346934,\n",
       " 553788,\n",
       " 572226,\n",
       " 11099,\n",
       " 258911,\n",
       " 451431,\n",
       " 525170,\n",
       " 127865,\n",
       " 355197,\n",
       " 426878,\n",
       " 326555,\n",
       " 476065,\n",
       " 91045,\n",
       " 201637,\n",
       " 504747,\n",
       " 11181,\n",
       " 465862,\n",
       " 357322,\n",
       " 510955,\n",
       " 216051,\n",
       " 209917,\n",
       " 252927,\n",
       " 5123,\n",
       " 580613,\n",
       " 85007,\n",
       " 424975,\n",
       " 324638,\n",
       " 125983,\n",
       " 205866,\n",
       " 283698,\n",
       " 58425,\n",
       " 400453,\n",
       " 422998,\n",
       " 56405,\n",
       " 246876,\n",
       " 230501,\n",
       " 128140,\n",
       " 472216,\n",
       " 195750,\n",
       " 580778,\n",
       " 578736,\n",
       " 85183,\n",
       " 89293,\n",
       " 117988,\n",
       " 515303,\n",
       " 281837,\n",
       " 62706,\n",
       " 204020,\n",
       " 242946,\n",
       " 202001,\n",
       " 552217,\n",
       " 365851,\n",
       " 576820,\n",
       " 228676,\n",
       " 255315,\n",
       " 419158,\n",
       " 11625,\n",
       " 566634,\n",
       " 52591,\n",
       " 468337,\n",
       " 134518,\n",
       " 77181,\n",
       " 322944,\n",
       " 564612,\n",
       " 54679,\n",
       " 71072,\n",
       " 105912,\n",
       " 366031,\n",
       " 454102,\n",
       " 536038,\n",
       " 136680,\n",
       " 421361,\n",
       " 554500,\n",
       " 462345,\n",
       " 230936,\n",
       " 513567,\n",
       " 198176,\n",
       " 99874,\n",
       " 200267,\n",
       " 276055,\n",
       " 341603,\n",
       " 73333,\n",
       " 3716,\n",
       " 401028,\n",
       " 315018,\n",
       " 153231,\n",
       " 93853,\n",
       " 560804,\n",
       " 403122,\n",
       " 22213,\n",
       " 407260,\n",
       " 577277,\n",
       " 100098,\n",
       " 165643,\n",
       " 149268,\n",
       " 393004,\n",
       " 411438,\n",
       " 124760,\n",
       " 116574,\n",
       " 14175,\n",
       " 548703,\n",
       " 333665,\n",
       " 442214,\n",
       " 305000,\n",
       " 393068,\n",
       " 319350,\n",
       " 200572,\n",
       " 526222,\n",
       " 233370,\n",
       " 575389,\n",
       " 542634,\n",
       " 403378,\n",
       " 42944,\n",
       " 147415]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woman_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5726126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_woman:  117\n",
      "num_man:  366\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize counters for woman and man synonyms\n",
    "image_results = {}\n",
    "woman_counter = 0\n",
    "man_counter = 0\n",
    "#woman_imgs_ = []\n",
    "#man_imgs = []\n",
    "\n",
    "# Iterate through all the image IDs and extract captions\n",
    "for img_id in chunck_val:  # Removed the [:1] so all images are processed\n",
    "    skip_image = False  # Flag to skip the current image if both types of synonyms are found\n",
    "    \n",
    "    # Get annotation IDs for the current image_id\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    \n",
    "    # Load the annotations for the current annotation IDs\n",
    "    annotations = coco.loadAnns(ann_ids)\n",
    "    \n",
    "    woman_found = False\n",
    "    man_found = False\n",
    "    caption = None\n",
    "    \n",
    "    # Iterate through the annotations and extract captions\n",
    "    for annotation in annotations:\n",
    "        caption = annotation['caption']\n",
    "        #print(f\"Image ID {img_id}: {caption}\")  # Print each caption\n",
    "        \n",
    "        # Check if the caption contains any woman synonym\n",
    "        if contains_synonym(caption, woman_word_list_synonyms):\n",
    "            woman_found = True\n",
    "\n",
    "        \n",
    "        # Check if the caption contains any man synonym\n",
    "        if contains_synonym(caption, man_word_list_synonyms):\n",
    "            man_found = True\n",
    "\n",
    "        \n",
    "        # If both woman and man synonyms are found in the same image, skip to next image\n",
    "        if woman_found and man_found:\n",
    "\n",
    "            skip_image = True\n",
    "            break\n",
    "    \n",
    "    if not skip_image:\n",
    "        if woman_found:\n",
    "            woman_counter += 1\n",
    "            woman_imgs.append(img_id)\n",
    "        elif man_found:\n",
    "            man_counter += 1\n",
    "            man_imgs.append(img_id)\n",
    "\n",
    "print('num_woman: ', woman_counter)\n",
    "print('num_man: ', man_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fceb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_imgs_final = woman_imgs[:188] \n",
    "man_imgs_final = man_imgs[:188] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e573101",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/mimer/NOBACKUP/groups/ulio_inverse/ds-project/GALS/data/COCO/COCO_gender/balanced_split/test_man_imgs_final.txt'\n",
    "\n",
    "# Save the list to the text file\n",
    "with open(file_path, 'w') as file:\n",
    "    for img_id in man_imgs_final:\n",
    "        file.write(f\"{img_id}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prob2)",
   "language": "python",
   "name": "prob2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
